image:
  repository: memgraphacrha.azurecr.io/memgraph/memgraph
    #tag: 2.22.0_23_8cb3c39c21
  tag: 2.22.0_30_8a58da1477
  pullPolicy: IfNotPresent

env:
  MEMGRAPH_ENTERPRISE_LICENSE: "<your-license>"
  MEMGRAPH_ORGANIZATION_NAME: "<your-organization-name>"

storage:
  libPVCSize: "1Gi"
  libStorageAccessMode: "ReadWriteOnce"
  # By default the name of the storage class isn't set which means that the default storage class will be used.
  # If you set any name, the storage class with such name must exist.
  libStorageClassName:
  logPVCSize: "1Gi"
  logStorageAccessMode: "ReadWriteOnce"
  logStorageClassName:

ports:
  boltPort: 7687
  managementPort: 10000
  replicationPort: 20000
  coordinatorPort: 12000

externalAccessConfig:
  dataInstance:
    serviceType: "NodePort"
  coordinator:
    serviceType: "NodePort"

# Affinity controls the scheduling of the memgraph-high-availability pods.
# By default data pods will avoid being scheduled on the same node as other data pods,
# and coordinator pods will avoid being scheduled on the same node as other coordinator pods.
# Deployment won't fail if there is no sufficient nodes.
affinity:
  # The unique affinity, will schedule the pods on different nodes in the cluster.
  # This means coordinators and data nodes will not be scheduled on the same node. If there are more pods than nodes, deployment will fail.
  unique: false
  # The parity affinity, will enable scheduling of the pods on the same node, but with the rule that one node can host pair made of coordinator and data node.
  # This means each node can have max two pods, one coordinator and one data node. If not sufficient nodes, deployment will fail.
  parity: false
  # The nodeSelection affinity, will enable scheduling of the pods on the nodes with specific labels. So the coordinators will be scheduled on the nodes with label coordinator-node and data nodes will be scheduled on the nodes with label data-node. If not sufficient nodes, deployment will fail.
  nodeSelection: false
  roleLabelKey: "role"
  dataNodeLabelValue: "data-node"
  coordinatorNodeLabelValue: "coordinator-node"

# If you are experiencing issues with the sysctlInitContainer, you can disable it here.
# This is made to increase the max_map_count, necessary for high memory loads in Memgraph
# If you are experiencing crashing pod with the: Max virtual memory areas vm.max_map_count is too low
# you can increase the maxMapCount value.
sysctlInitContainer:
  enabled: true
  maxMapCount: 262144

secrets:
  enabled: false
  name: memgraph-secrets
  userKey: USER
  passwordKey: PASSWORD

data:
- id: "0"
  args:
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--log-file=/var/log/memgraph/memgraph.log"

- id: "1"
  args:
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--log-file=/var/log/memgraph/memgraph.log"

coordinators:
- id: "1"
  args:
  - "--coordinator-id=1"
  - "--coordinator-port=12000"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--coordinator-hostname=memgraph-coordinator-1.default.svc.cluster.local"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--nuraft-log-file=/var/log/memgraph/memgraph.log"

- id: "2"
  args:
  - "--coordinator-id=2"
  - "--coordinator-port=12000"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--coordinator-hostname=memgraph-coordinator-2.default.svc.cluster.local"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--nuraft-log-file=/var/log/memgraph/memgraph.log"

- id: "3"
  args:
  - "--coordinator-id=3"
  - "--coordinator-port=12000"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--coordinator-hostname=memgraph-coordinator-3.default.svc.cluster.local"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--nuraft-log-file=/var/log/memgraph/memgraph.log"
