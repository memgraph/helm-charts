# Default values for memgraph-high-availability.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.|

memgraph:
  image:
    repository: memgraph/memgraph
    tag: 2.18.1
    pullPolicy: IfNotPresent
  env:
    MEMGRAPH_ENTERPRISE_LICENSE: "<your-license>"
    MEMGRAPH_ORGANIZATION_NAME: "<your-organization-name>"
  probes:
    startup:
      failureThreshold: 30
      periodSeconds: 10
    readiness:
      initialDelaySeconds: 5
      periodSeconds: 5
    liveness:
      initialDelaySeconds: 30
      periodSeconds: 10
  data:
    volumeClaim:
      storagePVCClassName: ""
      storagePVC: true
      storagePVCSize: "1Gi"
      logPVCClassName: ""
      logPVC: true
      logPVCSize: "256Mi"
  coordinators:
    volumeClaim:
      storagePVCClassName: ""
      storagePVC: true
      storagePVCSize: "1Gi"
      logPVCClassName: ""
      logPVC: true
      logPVCSize: "256Mi"

# Affinity controls the scheduling of the memgraph-high-availability pods, by disabling affinity, pods will be schedule on any node in the cluster, no matter if they are on the same node or not.
# The default unique affinity, will schedule the pods on different nodes in the cluster. This means coordinators and data nodes will not be scheduled on the same node, this is the most common use case for high availability. If not sufficient nodes, deployment will fail.
# The parity affinity, will enable scheduling of the pods on the same node, but with the rule that one node can host pair made of coordinator and data node. This means each node can have max two pods, one coordinator and one data node. If not sufficient nodes, deployment will fail.
# The paritySoft affinity, will enable scheduling of the pods on the same node, but with the rule that one node can host pair made of coordinator and data node. If not sufficent nodes, deployment won't fail, but will try to schedule as many pods by the rule as possible.
# The nodeSelection affinity, will enable scheduling of the pods on the nodes with specific labels. So the coordinators will be scheduled on the nodes with label coordinator-node and data nodes will be scheduled on the nodes with label data-node. The pods will prefer to spread on multiple data and coordinator nodes.
affinity:
  enabled: true
  unique: true
  parity: false
  paritySoft: false
  nodeSelection: false
  dataNodeLabel: "data-node"
  coordinatorNodeLabel: "coordinator-node"

data:
- id: "0"
  boltPort: 7687
  managementPort: 10000
  replicationPort: 20000
  args:
  - "--experimental-enabled=high-availability"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--log-file=/var/log/memgraph/memgraph.log"

- id: "1"
  boltPort: 7687
  managementPort: 10000
  replicationPort: 20000
  args:
  - "--experimental-enabled=high-availability"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--log-file=/var/log/memgraph/memgraph.log"

coordinators:
- id: "1"
  boltPort: 7687
  managementPort: 10000
  coordinatorPort: 12000
  args:
  - "--experimental-enabled=high-availability"
  - "--coordinator-id=1"
  - "--coordinator-port=12000"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--coordinator-hostname=memgraph-coordinator-1.default.svc.cluster.local"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--nuraft-log-file=/var/log/memgraph/memgraph.log"

- id: "2"
  boltPort: 7687
  managementPort: 10000
  coordinatorPort: 12000
  args:
  - "--experimental-enabled=high-availability"
  - "--coordinator-id=2"
  - "--coordinator-port=12000"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--coordinator-hostname=memgraph-coordinator-2.default.svc.cluster.local"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--nuraft-log-file=/var/log/memgraph/memgraph.log"

- id: "3"
  boltPort: 7687
  managementPort: 10000
  coordinatorPort: 12000
  args:
  - "--experimental-enabled=high-availability"
  - "--coordinator-id=3"
  - "--coordinator-port=12000"
  - "--management-port=10000"
  - "--bolt-port=7687"
  - "--also-log-to-stderr"
  - "--log-level=TRACE"
  - "--coordinator-hostname=memgraph-coordinator-3.default.svc.cluster.local"
  - "--log-file=/var/log/memgraph/memgraph.log"
  - "--nuraft-log-file=/var/log/memgraph/memgraph.log"
